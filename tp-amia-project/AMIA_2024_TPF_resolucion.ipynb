{
 "cells": [
  {
   "cell_type": "code",
   "id": "a3cf4012d20e8b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:02.135120Z",
     "start_time": "2024-08-15T03:59:01.110713Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from utils import ClassEncoder\n",
    "from datasets import get_iris_dataset\n",
    "X_full_iris, y_full_iris = get_iris_dataset()\n",
    "print(X_full_iris.shape)\n",
    "print(y_full_iris.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 1)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b83ef9c952b426af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:02.163463Z",
     "start_time": "2024-08-15T03:59:02.157446Z"
    }
   },
   "source": [
    "def print_distribution(_y):\n",
    "    encoder = ClassEncoder()\n",
    "    encoded_y = encoder.fit_transform(_y) # convert class to number (encode)\n",
    "    print('Distribución de clases:')\n",
    "    distribution = np.bincount(encoded_y.flatten())/len(encoded_y)\n",
    "    for class_name, value in zip(encoder.names, distribution):\n",
    "        print(f'{class_name}: {value:.4f}')\n",
    "\n",
    "print_distribution(y_full_iris)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de clases:\n",
      "setosa: 0.3333\n",
      "versicolor: 0.3333\n",
      "virginica: 0.3333\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "767fec1ed536e345",
   "metadata": {},
   "source": [
    "Se observa que el dataset Iris se encuentra balanceado, es decir que no hay alguna preponderancia de alguna de las clases por sobre las demás."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6de399ab132490",
   "metadata": {},
   "source": [
    "### 1) QDA Entrenado con:  probabilidades a priori uniforme y  una clase con probabilidad 0.9, las demás 0.05 ( 3 combinaciones)"
   ]
  },
  {
   "cell_type": "code",
   "id": "4fbc6833c5a1e73b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:02.420879Z",
     "start_time": "2024-08-15T03:59:02.361316Z"
    }
   },
   "source": [
    "from utils import split_transpose, QDA, accuracy\n",
    "\n",
    "def priori_test(dataset):\n",
    "    X_full, y_full = dataset\n",
    "    a_priori_A = [1/3, 1/3, 1/3] # modelo 0\n",
    "    a_priori_B_1 = [0.9, 0.05, 0.05] # modelo 1\n",
    "    a_priori_B_2 = [0.05, 0.9, 0.05] # modelo 2\n",
    "    a_priori_B_3 = [0.05, 0.05, 0.9] # modelo 3\n",
    "    \n",
    "    a_priori_list = [a_priori_A, a_priori_B_1, a_priori_B_2, a_priori_B_3]\n",
    "    # from utils import QDA\n",
    "    # rng_seed = 6543\n",
    "    train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, 6543)\n",
    "    \n",
    "    for i,_a_priori in enumerate(a_priori_list):\n",
    "        model = QDA()\n",
    "        model.fit(train_x, train_y, _a_priori)\n",
    "        print('A prioris:')\n",
    "        print(\",\".join([f' {class_name}:{p:.3f} ' for class_name, p in zip(model.encoder.names, _a_priori)]))\n",
    "        train_acc = accuracy(train_y, model.predict(train_x))\n",
    "        test_acc = accuracy(test_y, model.predict(test_x))\n",
    "        print(f\"[Model {i}] Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")\n",
    "        # print('\\n')\n",
    "\n",
    "priori_test(get_iris_dataset())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A prioris:\n",
      " setosa:0.333 , versicolor:0.333 , virginica:0.333 \n",
      "[Model 0] Train (apparent) error is 0.0222 while test error is 0.0167\n",
      "A prioris:\n",
      " setosa:0.900 , versicolor:0.050 , virginica:0.050 \n",
      "[Model 1] Train (apparent) error is 0.0222 while test error is 0.0167\n",
      "A prioris:\n",
      " setosa:0.050 , versicolor:0.900 , virginica:0.050 \n",
      "[Model 2] Train (apparent) error is 0.0333 while test error is 0.0000\n",
      "A prioris:\n",
      " setosa:0.050 , versicolor:0.050 , virginica:0.900 \n",
      "[Model 3] Train (apparent) error is 0.0333 while test error is 0.0500\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "7c22fb5b463cbbbd",
   "metadata": {},
   "source": [
    "A partir de estos datos y dejando de lado cuál es la verdadera distribución, se podrían hacer las siguientes suposiciones:\n",
    "- El modelo 0 y el modelo 1 parecerían cometer el mismo grado de error al hacer dichas suposiciones sobre los priors (la clase setosa no parece verse afectada).\n",
    "- El modelo 2 parecería sobreajustar (hay overfitting) a los datos de test.\n",
    "- El modelo 3 tiene un mayor error tanto en el entrenamiento como en la prueba, lo que indica que no logra generalizar y que dichos priors tienen un efecto detrimental en la performance del modelo.\n",
    "- Vemos que en los casos 2 y 3 aumenta el error en train y esto podria tener que ver con que la priori de setosa es baja.\n",
    "\n",
    "Se utilizará la versión de QDA provista por SKLearn (a pesar de las diferencias en su implementación) a modo de evaluar cualitativamente el efecto de los priors sobre éste dataset y confirmar las suposiciones ya mencionadas, utilizando Cross Validation:"
   ]
  },
  {
   "cell_type": "code",
   "id": "5eb276f0-7635-4c7f-9eb5-7017aa69781c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:02.553692Z",
     "start_time": "2024-08-15T03:59:02.452767Z"
    }
   },
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def priori_test_cross_val(dataset):\n",
    "    X_full, y_full = dataset\n",
    "    a_priori_A = [1/3, 1/3, 1/3] # modelo 0\n",
    "    a_priori_B_1 = [0.9, 0.05, 0.05] # modelo 1\n",
    "    a_priori_B_2 = [0.05, 0.9, 0.05] # modelo 2\n",
    "    a_priori_B_3 = [0.05, 0.05, 0.9] # modelo 3\n",
    "        \n",
    "    a_priori_list = [a_priori_A, a_priori_B_1, a_priori_B_2, a_priori_B_3]\n",
    "    \n",
    "    for i, _priors in enumerate(a_priori_list):\n",
    "        qda = QuadraticDiscriminantAnalysis(priors=_priors) #utilizo QDA de sklearn para poder utilizar cross_val_score sin problemas\n",
    "        cv_scores = cross_val_score(qda, X_full, y_full.flatten(), cv=5)  # 5-fold cross-validation\n",
    "        print(f\"Accuracy with priors {_priors}: {cv_scores.mean()} ± {cv_scores.std()} for model {i}\")\n",
    "\n",
    "priori_test_cross_val(get_iris_dataset())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with priors [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]: 0.9800000000000001 ± 0.02666666666666666 for model 0\n",
      "Accuracy with priors [0.9, 0.05, 0.05]: 0.9800000000000001 ± 0.02666666666666666 for model 1\n",
      "Accuracy with priors [0.05, 0.9, 0.05]: 0.9733333333333334 ± 0.024944382578492935 for model 2\n",
      "Accuracy with priors [0.05, 0.05, 0.9]: 0.9666666666666668 ± 0.036514837167011066 for model 3\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "13072daf-0fbe-497f-8f5e-91b3e3c53547",
   "metadata": {},
   "source": [
    "Aquí se ve que el resultado del modelo 2 anterior no era representativo. La precisión es ligeramente menor que en los modelos anteriores, lo que podría indicar que las muestras de la segunda clase no son tan fácilmente separables como las de la primera clase.\n",
    "\n",
    "En cuanto al modelo 3: presenta la precisión más baja y la desviación estándar más alta, lo que podría sugerir que la tercera clase es la más difícil de clasificar correctamente o que el sesgo hacia esa clase introduce más incertidumbre en las predicciones.\n",
    "\n",
    "Con respecto al modelo 0 y 1: cabe la posibilidad de pensar que por ejemplo, las características que definen la primera clase son muy distintas, el modelo podría seguir clasificando correctamente la mayoría de las instancias de esa clase, incluso si los priors cambian. Ello querría decir que dicha clase es fácilmente separable de las demás. Para obtener una visión más clara de qué sucediendo entre el modelo 1 y el 0, se puede ver la matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "id": "ceafba33-5913-469f-8826-05d30f7b8ba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:02.619900Z",
     "start_time": "2024-08-15T03:59:02.590398Z"
    }
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(model, test_x, test_y):\n",
    "    test_preds_0 = model.predict(test_x)\n",
    "    return confusion_matrix(test_y.T, test_preds_0.T)\n",
    "\n",
    "\n",
    "X_full, y_full = get_iris_dataset()\n",
    "a_priori_A = [1/3, 1/3, 1/3] # modelo 0\n",
    "a_priori_B_1 = [0.9, 0.05, 0.05] # modelo 1\n",
    "a_priori_B_2 = [0.05, 0.9, 0.05] # modelo 2\n",
    "a_priori_B_3 = [0.05, 0.05, 0.9] # modelo 3\n",
    "\n",
    "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, 6543)\n",
    "\n",
    "model = QDA()\n",
    "\n",
    "model.fit(train_x, train_y, a_priori_A)\n",
    "print(\"Matriz de confusión de prueba para el modelo 0:\")\n",
    "print(get_cm(model, test_x, test_y))\n",
    "\n",
    "model.fit(train_x, train_y, a_priori_B_1)\n",
    "print(\"Matriz de confusión de prueba para el modelo 1:\")\n",
    "print(get_cm(model, test_x, test_y))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión de prueba para el modelo 0:\n",
      "[[23  0  0]\n",
      " [ 0 20  1]\n",
      " [ 0  0 16]]\n",
      "Matriz de confusión de prueba para el modelo 1:\n",
      "[[23  0  0]\n",
      " [ 0 20  1]\n",
      " [ 0  0 16]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "0f8091cf-a134-4179-9f00-70a9588c29cd",
   "metadata": {},
   "source": [
    "El hecho de que el modelo 1, con priors sesgados ([0.9, 0.05, 0.05]), produzca una matriz de confusión similar indica que la información proporcionada por los datos (en este caso de la clase 1) es lo suficientemente fuerte como para compensar el sesgo de los priors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4691e2c3b5a69f5",
   "metadata": {},
   "source": [
    "## 2) Repetir punto 1 para el dataset penguin"
   ]
  },
  {
   "cell_type": "code",
   "id": "4263ac5f7974baf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:02.919033Z",
     "start_time": "2024-08-15T03:59:02.658485Z"
    }
   },
   "source": [
    "from datasets import get_penguins\n",
    "X_full_penguin, y_full_penguin = get_penguins()\n",
    "\n",
    "print_distribution(y_full_penguin)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de clases:\n",
      "Adelie: 0.4415\n",
      "Chinstrap: 0.1988\n",
      "Gentoo: 0.3596\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "7a07fb8018fb6aca",
   "metadata": {},
   "source": [
    "Se puede observar este dataset no está balanceado con respecto a la cantidad de datos por clase."
   ]
  },
  {
   "cell_type": "code",
   "id": "44318e84b2b475f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:03.019816Z",
     "start_time": "2024-08-15T03:59:02.939449Z"
    }
   },
   "source": [
    "priori_test(get_penguins())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A prioris:\n",
      " Adelie:0.333 , Chinstrap:0.333 , Gentoo:0.333 \n",
      "[Model 0] Train (apparent) error is 0.0098 while test error is 0.0073\n",
      "A prioris:\n",
      " Adelie:0.900 , Chinstrap:0.050 , Gentoo:0.050 \n",
      "[Model 1] Train (apparent) error is 0.0195 while test error is 0.0219\n",
      "A prioris:\n",
      " Adelie:0.050 , Chinstrap:0.900 , Gentoo:0.050 \n",
      "[Model 2] Train (apparent) error is 0.0098 while test error is 0.0219\n",
      "A prioris:\n",
      " Adelie:0.050 , Chinstrap:0.050 , Gentoo:0.900 \n",
      "[Model 3] Train (apparent) error is 0.0098 while test error is 0.0073\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "a44c4be3-c9b4-4961-98d1-cce5ee6e7dbf",
   "metadata": {},
   "source": [
    "Los modelos que mejor generalizan son el 0 y el 3. Parece ser un caso análogo al de Iris en cuanto a lo que sucede con diferentes priors. Se intentará verificar a continuación si esto es así:"
   ]
  },
  {
   "cell_type": "code",
   "id": "44fc8bf0-47e6-49ac-a9ed-be8dc0aac115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:03.127129Z",
     "start_time": "2024-08-15T03:59:03.049014Z"
    }
   },
   "source": [
    "priori_test_cross_val(get_penguins())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with priors [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]: 0.9882779198635976 ± 0.010993807100854342 for model 0\n",
      "Accuracy with priors [0.9, 0.05, 0.05]: 0.9824381926683717 ± 0.005925745287640952 for model 1\n",
      "Accuracy with priors [0.05, 0.9, 0.05]: 0.9589514066496164 ± 0.025287380244108995 for model 2\n",
      "Accuracy with priors [0.05, 0.05, 0.9]: 0.9882779198635976 ± 0.010993807100854342 for model 3\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "d31fd69f-9450-4896-bd05-152e3f286175",
   "metadata": {},
   "source": [
    "Se puede ahora notar nuevamente que los modelos 1 (aunque por muy poco) y 2 son los que peor performan."
   ]
  },
  {
   "cell_type": "code",
   "id": "8ff41f08-0b70-41cb-9202-6e841535310c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:03.231913Z",
     "start_time": "2024-08-15T03:59:03.181980Z"
    }
   },
   "source": [
    "X_full, y_full = get_penguins()\n",
    "\n",
    "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, 6543)\n",
    "\n",
    "model = QDA()\n",
    "\n",
    "model.fit(train_x, train_y, a_priori_A)\n",
    "print(\"Matriz de confusión de prueba para el modelo 0:\")\n",
    "print(get_cm(model, test_x, test_y))\n",
    "\n",
    "model.fit(train_x, train_y, a_priori_B_1)\n",
    "print(\"Matriz de confusión de prueba para el modelo 1:\")\n",
    "print(get_cm(model, test_x, test_y))\n",
    "\n",
    "model.fit(train_x, train_y, a_priori_B_2)\n",
    "print(\"Matriz de confusión de prueba para el modelo 2:\")\n",
    "print(get_cm(model, test_x, test_y))\n",
    "\n",
    "model.fit(train_x, train_y, a_priori_B_3)\n",
    "print(\"Matriz de confusión de prueba para el modelo 3:\")\n",
    "print(get_cm(model, test_x, test_y))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión de prueba para el modelo 0:\n",
      "[[67  0  0]\n",
      " [ 1 28  0]\n",
      " [ 0  0 41]]\n",
      "Matriz de confusión de prueba para el modelo 1:\n",
      "[[67  0  0]\n",
      " [ 3 26  0]\n",
      " [ 0  0 41]]\n",
      "Matriz de confusión de prueba para el modelo 2:\n",
      "[[64  3  0]\n",
      " [ 0 29  0]\n",
      " [ 0  0 41]]\n",
      "Matriz de confusión de prueba para el modelo 3:\n",
      "[[67  0  0]\n",
      " [ 1 28  0]\n",
      " [ 0  0 41]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "86ae2fde-b3b1-4b48-94e4-5d86fc6da0c1",
   "metadata": {},
   "source": [
    "De aquí se puede concluir que existe una relación de compromiso entre clasificar correctamente las muestras de la primera y segunda clase. El límite de decisión que genera un error 0 requiera probablemente de mayor complejidad (trayendo aparejado un posible problema de overfitting) o siendo imposible de lograr.\n",
    "\n",
    "El modelo 0 cuyos priors son los que más se asemejan a los reales, es uno de los que mejor performa. El modelo 3 da resultados similares; ello se debe a que la tercera clase es evidentemente es fácilmente separable de las demás (puesto a que todos los modelos, a pesar de la abismal diferencia entre priors, han sido capaz de clasificarlos correctamente) y por lo tanto, la información proporcionada por los datos es lo suficientemente fuerte como para compensar el sesgo del prior (análogo al dataset anterior)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b5275361fad988",
   "metadata": {},
   "source": [
    "### 3) Implementar LDA"
   ]
  },
  {
   "cell_type": "code",
   "id": "9bf258ed0b1a69a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:03.265151Z",
     "start_time": "2024-08-15T03:59:03.257774Z"
    }
   },
   "source": [
    "from utils import BaseBayesianClassifier, inv, det\n",
    "\n",
    "class LDA(BaseBayesianClassifier):\n",
    "    # Utiliza una matriz de covarianza ponderada para evitar que clases con mayor cantidad de muestras tengan mayor influencia en su valor final. \n",
    "    # Al ponderar, se intenta que la variabilidad dentro de la clase más pequeña también sea tenida en cuenta de manera proporcional.\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # Inicializa la matriz de covarianza ponderada\n",
    "    \n",
    "    cov_matrix = np.zeros((X.shape[0], X.shape[0]))\n",
    "    \n",
    "    # Suma la covarianza de cada clase ponderada por su tamaño\n",
    "    for idx in range(len(self.log_a_priori)):\n",
    "        X_class = X[:, y.flatten() == idx]\n",
    "        cov_matrix += np.cov(X_class, bias=True) * X_class.shape[1] # estimador de máxima verosimilitud\n",
    "        # cov_matrix += np.cov(X_class, bias=False) * (X_class.shape[1] - 1) # estimador insesgado\n",
    "\n",
    "    # Divide por el número total de muestras\n",
    "    cov_matrix /= X.shape[1] # estimador de máxima verosimilitud\n",
    "    # # Divide por el número total de muestras (menos 1 para corrección de Bessel)\n",
    "    # cov_matrix /= (X.shape[1] - len(self.log_a_priori)) # estimador insesgado\n",
    "    \n",
    "    # Calcula la inversa de la matriz de covarianza ponderada\n",
    "    self.inv_cov = inv(cov_matrix)\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True) for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "    return 0.5*np.log(det(self.inv_cov)) -0.5 * unbiased_x.T @ self.inv_cov @ unbiased_x"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "4b3349622de5cd6e",
   "metadata": {},
   "source": [
    "Se comparar LDA vs QDA (Sin multiples prioris, es decir que se estimen a partir de los datos) con los dos datasets:"
   ]
  },
  {
   "cell_type": "code",
   "id": "5afccf7c21a27a1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:03.375025Z",
     "start_time": "2024-08-15T03:59:03.302947Z"
    }
   },
   "source": [
    "for dataset_name, dataset in zip(['iris', 'penguins'], [get_iris_dataset(), get_penguins()]):\n",
    "    for model_name, curr_model in zip(['QDA', 'LDA'], [QDA, LDA]):\n",
    "        model = curr_model()\n",
    "        x_full, y_full = dataset\n",
    "        train_x, train_y, test_x, test_y = split_transpose(x_full, y_full, 0.4, 6543)\n",
    "        model.fit(train_x, train_y)\n",
    "        train_acc = accuracy(train_y, model.predict(train_x))\n",
    "        test_acc = accuracy(test_y, model.predict(test_x))\n",
    "        print(f\"[Dataset={dataset_name}][Model={model_name}] train err {1-train_acc:.4f}, test err {1-test_acc:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset=iris][Model=QDA] train err 0.0111, test err 0.0167\n",
      "[Dataset=iris][Model=LDA] train err 0.0222, test err 0.0167\n",
      "[Dataset=penguins][Model=QDA] train err 0.0146, test err 0.0146\n",
      "[Dataset=penguins][Model=LDA] train err 0.0098, test err 0.0146\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "e2b0fe07de94eadb",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Según lo observado, LDA es un mejor modelo cuando se tiene de un número limitado de datos, mientras que QDA necesita de un tamaño mayor de datos y una mayor variabilidad entre clases para tener una mayor eficacia. Por lo tanto, en situaciones donde se cuenta con poca cantidad de datos o las clases no están bien representadas, LDA puede ser la mejor elección.\n",
    "\n",
    "### 4) Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Las conclusiones previas se mantienen?"
   ]
  },
  {
   "cell_type": "code",
   "id": "7a470151e2b2562f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:03.585168Z",
     "start_time": "2024-08-15T03:59:03.408051Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for dataset_name,dataset in zip(['iris', 'penguins'],[get_iris_dataset(), get_penguins()]):\n",
    "    for model_name, curr_model in zip(['QDA', 'LDA'],[QDA, LDA]):\n",
    "        for seed in [6543, 5501,125]:\n",
    "            model = curr_model()\n",
    "            x_full, y_full = dataset\n",
    "            train_x, train_y, test_x, test_y = split_transpose(x_full, y_full,test_sz=0.4, random_state=seed)\n",
    "            model.fit(train_x, train_y)\n",
    "            train_acc = accuracy(train_y, model.predict(train_x))\n",
    "            test_acc = accuracy(test_y, model.predict(test_x))\n",
    "            # print(f\"[Dataset={dataset_name}][Model={model_name}] train err {1-train_acc:.4f}, test err {1-test_acc:.4f}\")\n",
    "            row = {\n",
    "                'Dataset': dataset_name,\n",
    "                'Model': model_name,\n",
    "                'seed': seed,\n",
    "                'Error (train)': 1-train_acc,\n",
    "                'Error (test)': 1-test_acc,\n",
    "            }\n",
    "            \n",
    "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "print(df)\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Dataset Model  seed  Error (train)  Error (test)\n",
      "0       iris   QDA  6543       0.011111      0.016667\n",
      "1       iris   QDA  5501       0.022222      0.016667\n",
      "2       iris   QDA   125       0.022222      0.016667\n",
      "3       iris   LDA  6543       0.022222      0.016667\n",
      "4       iris   LDA  5501       0.022222      0.016667\n",
      "5       iris   LDA   125       0.011111      0.016667\n",
      "6   penguins   QDA  6543       0.014634      0.014599\n",
      "7   penguins   QDA  5501       0.014634      0.007299\n",
      "8   penguins   QDA   125       0.009756      0.014599\n",
      "9   penguins   LDA  6543       0.009756      0.014599\n",
      "10  penguins   LDA  5501       0.009756      0.014599\n",
      "11  penguins   LDA   125       0.014634      0.007299\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "9b8c9f73-e241-4326-aec7-576bf4d47d14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:38.394522Z",
     "start_time": "2024-08-13T02:47:38.391794Z"
    }
   },
   "source": [
    "### Conclusion\n",
    "Las conclusiones iniciales son válidas: LDA sigue siendo la mejor elección en situaciones con datos limitados o clases desbalanceadas, mientras que QDA podría beneficiarse de un mayor volumen de datos para maximizar su eficacia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a8736a93b6bc1",
   "metadata": {},
   "source": [
    "### 5) Tensorized QDA vs QDA"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9d5e4c3036eca76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:03.626931Z",
     "start_time": "2024-08-15T03:59:03.615993Z"
    }
   },
   "source": [
    "from utils import TensorizedQDA\n",
    "\n",
    "x_full, y_full = get_iris_dataset()\n",
    "train_x, train_y, test_x, test_y = split_transpose(x_full, y_full,test_sz=0.4, random_state=6543)\n",
    "\n",
    "tqda = TensorizedQDA()\n",
    "tqda.fit(train_x, train_y)\n",
    "train_acc = accuracy(train_y, tqda.predict(train_x))\n",
    "test_acc = accuracy(test_y, tqda.predict(test_x))\n",
    "\n",
    "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0111 while test error is 0.0167\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "ab0347e9246ad3bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:12.109216Z",
     "start_time": "2024-08-15T03:59:03.672082Z"
    }
   },
   "source": [
    "%%timeit\n",
    "\n",
    "tqda.predict(test_x)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.03 ms ± 25.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "76f3bd3c-9bbe-4285-ba7f-fa3c8db124de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:12.152897Z",
     "start_time": "2024-08-15T03:59:12.147328Z"
    }
   },
   "source": [
    "tqda_predictions = tqda.predict(test_x)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "7970b3217fa7540",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:12.213097Z",
     "start_time": "2024-08-15T03:59:12.200744Z"
    }
   },
   "source": [
    "qda = QDA()\n",
    "qda.fit(train_x, train_y)\n",
    "train_acc = accuracy(train_y, qda.predict(train_x))\n",
    "test_acc = accuracy(test_y, qda.predict(test_x))\n",
    "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0111 while test error is 0.0167\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "9fae0163b9d8c14f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:14.358279Z",
     "start_time": "2024-08-15T03:59:12.265075Z"
    }
   },
   "source": [
    "%%timeit\n",
    "\n",
    "qda.predict(test_x)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.56 ms ± 40.9 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "4f685564-8127-4da0-8872-a0aaa847936c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:49.379366Z",
     "start_time": "2024-08-13T02:47:49.377086Z"
    }
   },
   "source": [
    "Tensorized QDA aprovecha que varias de las operaciones que se tienen que realizar se pueden escribir de forma matricial, pudiendo realizarse de forma paralela. Esto se hace evidente en las siguientes líneas:\n",
    "\n",
    "```\n",
    "self.tensor_inv_cov = np.stack(self.inv_covs)\n",
    "self.tensor_means = np.stack(self.means)\n",
    "```\n",
    "\n",
    "Lo cual muestra que las cuentas serán hechas simultáneamente, de forma paralela, para todas las clases y luego se verifica en la firma de la función:\n",
    "```\n",
    "_predict_log_conditionals(self, x)\n",
    "```\n",
    "\n",
    "Ya que no es necesario especificar para qué clase será hecha dicha predicción.\n",
    "\n",
    "Además, es aproximadamente 3 veces más rápido que coincide con la cantidad de clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88475da94ee1081",
   "metadata": {},
   "source": [
    "## Faster QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1a87a-83e0-401a-8a9a-2f7535baf5ff",
   "metadata": {},
   "source": [
    "Si intentamos calcular todas las predicciones simultáneamente, terminamos calculando una matriz que contiene no solo la información de interés en la diagonal (auto-interacciones), sino también las interacciones cruzadas entre las observaciones.\n",
    "\n",
    "La diagonal de esta matriz representa los términos que se usarían en el cálculo individual de la función discriminante para cada observación.\n",
    "Los términos fuera de la diagonal representan interacciones entre diferentes observaciones, lo cual no es necesario si se quiere predecir para cada observación de forma independiente.\n",
    "Por lo tanto, para obtener las predicciones, podrías ignorar los términos fuera de la diagonal, pero la construcción de toda la matriz sigue siendo costosa en términos de tiempo y memoria."
   ]
  },
  {
   "cell_type": "code",
   "id": "a465a592ed10a61d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:14.423877Z",
     "start_time": "2024-08-15T03:59:14.418763Z"
    }
   },
   "source": [
    "from utils import TensorizedQDA\n",
    "class FasterQDAOptim1(TensorizedQDA):\n",
    "    \n",
    "    def _predict_log_conditionals(self, x):\n",
    "        # k: clases\n",
    "        # p: features\n",
    "        # n: observaciones\n",
    "        # x era de p x n\n",
    "        # despues de hacer fit -> x es un tensor de k x p x n (o sea mismo array, k veces)\n",
    "        # (se hace la prediccion para las k clases en conjunto)\n",
    "        unbiased_x = x - self.tensor_means\n",
    "        inner_prod = unbiased_x.transpose(0, 2, 1) @ self.tensor_inv_cov @ unbiased_x\n",
    "        # print(inner_prod.shape) # matrices cuadradas que pide mostrar la consigna       DESCOMENTARRRRR!!!!!!\n",
    "        # inner prod shape: (k, n, n) -> k matrices de n x n\n",
    "        # se debe de pensar que como es un producto de tensores => cada producto produce una matriz de n x n\n",
    "        \n",
    "        # dado un k fijo (estando posicionados en una clase)\n",
    "        # se puede imaginar que en cada matriz\n",
    "        # obtengo diagonal,\n",
    "        # ya que solo las que son consigo mismas son las que nos interesan (producto interno)\n",
    "        new_mat = np.array([np.diag(mat) for mat in inner_prod]) # (k, n)\n",
    "        # new_mat = np.sum(unbiased_x.transpose(0, 2, 1) @ self.tensor_inv_cov, axis=2)\n",
    "        # new_mat = np.sum(self.tensor_inv_cov@unbiased_x, axis=1)\n",
    "        itcov = np.log(det(self.tensor_inv_cov)) # (k,)\n",
    "        \n",
    "        # truco para no tener que crear un nuevo vector itcov de (k, n) para poder sumarlo\n",
    "        return 0.5 * itcov - 0.5 * new_mat.transpose() # (n, k)\n",
    "        \n",
    "    def predict(self, x):        \n",
    "        log_cond = self._predict_log_conditionals(x) # (n, k)\n",
    "        log_priors = self.log_a_priori #(k,)\n",
    "        y_hat = self.encoder.names[np.argmax(log_priors + log_cond, axis=1)]\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "fqda1 = FasterQDAOptim1()\n",
    "fqda1.fit(train_x, train_y)\n",
    "# fqda1.predict(train_x)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "c3085b7570f05c69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:18.655966Z",
     "start_time": "2024-08-15T03:59:14.465119Z"
    }
   },
   "source": [
    "%%timeit\n",
    "fqda1.predict(train_x)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.5 μs ± 869 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "3dc0df3b-ef4d-4226-ae31-78c664e48dd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:18.691218Z",
     "start_time": "2024-08-15T03:59:18.687111Z"
    }
   },
   "source": [
    "fqda1_predictions = fqda1.predict(test_x)\n",
    "print(fqda1_predictions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['versicolor' 'versicolor' 'virginica' 'setosa' 'virginica' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'virginica' 'versicolor'\n",
      " 'versicolor' 'virginica' 'setosa' 'setosa' 'versicolor' 'setosa' 'setosa'\n",
      " 'versicolor' 'setosa' 'versicolor' 'setosa' 'virginica' 'setosa'\n",
      " 'virginica' 'setosa' 'versicolor' 'setosa' 'versicolor' 'virginica'\n",
      " 'versicolor' 'setosa' 'versicolor' 'virginica' 'virginica' 'versicolor'\n",
      " 'virginica' 'versicolor' 'setosa' 'virginica' 'setosa' 'virginica'\n",
      " 'setosa' 'setosa' 'versicolor' 'virginica' 'virginica' 'setosa' 'setosa'\n",
      " 'setosa' 'virginica' 'virginica' 'versicolor' 'setosa' 'virginica'\n",
      " 'setosa' 'versicolor' 'setosa' 'setosa' 'setosa']\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "a133ce75ffb9867a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:18.751946Z",
     "start_time": "2024-08-15T03:59:18.745950Z"
    }
   },
   "source": [
    "from utils import TensorizedQDA\n",
    "class FasterQDAOptim2(TensorizedQDA):\n",
    "    \n",
    "    # def _predict_log_conditionals(self, x):\n",
    "    #     unbiased_x = x - self.tensor_means\n",
    "    #     # bug en consigna corregido seria -> diag(A dot prod B) = sum cols (B @ A^t)\n",
    "    #     new_mat = np.sum(self.tensor_inv_cov@unbiased_x, axis=1)\n",
    "    #     itcov = np.log(det(self.tensor_inv_cov)) # (k,)\n",
    "        \n",
    "    #     # truco para no tener que crear un nuevo vector itcov de (k, n) para poder sumarlo\n",
    "    #     return - 0.5 * itcov - 0.5 * new_mat.transpose() # (n, k)\n",
    "\n",
    "    def _predict_log_conditionals(self, x):\n",
    "        unbiased_x = x - self.tensor_means\n",
    "        Z = (unbiased_x.transpose(0, 2, 1) @ self.tensor_inv_cov)\n",
    "        new_mat = np.sum(Z.transpose(0, 2, 1) * unbiased_x, axis=1)\n",
    "        itcov = np.log(det(self.tensor_inv_cov)) # (k,)\n",
    "        \n",
    "        # truco para no tener que crear un nuevo vector itcov de (k, n) para poder sumarlo\n",
    "        return 0.5 * itcov - 0.5 * new_mat.transpose() # (n, k)\n",
    "        \n",
    "    def predict(self, x):        \n",
    "        log_cond = self._predict_log_conditionals(x) # (n, k)\n",
    "        log_priors = self.log_a_priori #(k,)\n",
    "        # print(log_priors + log_cond)\n",
    "        y_hat = self.encoder.names[np.argmax(log_priors + log_cond, axis=1)]\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "fqda2 = FasterQDAOptim2()\n",
    "fqda2.fit(train_x, train_y)\n",
    "# fqda2.predict(train_x)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "cbccb00d5898e229",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:21.515585Z",
     "start_time": "2024-08-15T03:59:18.805195Z"
    }
   },
   "source": [
    "%%timeit\n",
    "fqda2.predict(train_x)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.1 μs ± 144 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "139a73f1-eaf2-4394-800c-92e00f922bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:21.600499Z",
     "start_time": "2024-08-15T03:59:21.595600Z"
    }
   },
   "source": [
    "fqda2_predictions = fqda2.predict(test_x)\n",
    "print(fqda2_predictions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['versicolor' 'versicolor' 'virginica' 'setosa' 'virginica' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'virginica' 'versicolor'\n",
      " 'versicolor' 'virginica' 'setosa' 'setosa' 'versicolor' 'setosa' 'setosa'\n",
      " 'versicolor' 'setosa' 'versicolor' 'setosa' 'virginica' 'setosa'\n",
      " 'virginica' 'setosa' 'versicolor' 'setosa' 'versicolor' 'virginica'\n",
      " 'versicolor' 'setosa' 'versicolor' 'virginica' 'virginica' 'versicolor'\n",
      " 'virginica' 'versicolor' 'setosa' 'virginica' 'setosa' 'virginica'\n",
      " 'setosa' 'setosa' 'versicolor' 'virginica' 'virginica' 'setosa' 'setosa'\n",
      " 'setosa' 'virginica' 'virginica' 'versicolor' 'setosa' 'virginica'\n",
      " 'setosa' 'versicolor' 'setosa' 'setosa' 'setosa']\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "85566470-d7fd-445d-9187-b5ebd862315c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:21.715913Z",
     "start_time": "2024-08-15T03:59:21.712153Z"
    }
   },
   "source": "print(np.bincount(fqda1_predictions.flatten() == fqda2_predictions.flatten()))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 60]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "ce69e0a6-d2f3-4050-a997-4827768f5f03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:21.808977Z",
     "start_time": "2024-08-15T03:59:21.805172Z"
    }
   },
   "source": "print(np.bincount(tqda_predictions.flatten() == fqda2_predictions.flatten())) ",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 60]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:21.877211Z",
     "start_time": "2024-08-15T03:59:21.861780Z"
    }
   },
   "cell_type": "code",
   "source": "tqda._predict_custom(test_x)",
   "id": "d4611d67403501f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-167.68606827    0.89806493  -11.37774515]\n",
      "[-310.26617083    4.84443575   -0.42072607]\n",
      "[-391.25152539   -1.09796224    2.70052269]\n",
      "[   3.62296713  -52.74584521 -113.45417897]\n",
      "[-427.21142602   -2.32554356    2.94101411]\n",
      "[-346.36869081   -1.26424642   -2.69449688]\n",
      "[-224.62962626    4.61387291   -2.89202058]\n",
      "[-224.02781482    2.00126012   -8.59335235]\n",
      "[-231.19535868    5.38625066   -4.30841591]\n",
      "[-650.64637775   -2.30002693    2.87766313]\n",
      "[-3.19419483e+02 -1.73522721e-01 -1.82084322e+00]\n",
      "[-241.6476793     5.02706153   -4.3486513 ]\n",
      "[-1008.48079905   -22.17694322    -7.51139885]\n",
      "[  5.32995623 -37.79452599 -90.75014618]\n",
      "[   6.52254929  -46.46442567 -105.57409642]\n",
      "[-230.33580652    3.07200566   -7.0318846 ]\n",
      "[   1.32729158  -75.52098046 -149.65896307]\n",
      "[   5.77394601  -45.88750593 -107.05766076]\n",
      "[-296.97290043    4.25565011   -6.45363238]\n",
      "[  5.67276473 -34.51210807 -85.11455709]\n",
      "[-391.63661865    4.15175591   -2.43280053]\n",
      "[  1.03010515 -43.10896965 -95.52537296]\n",
      "[-468.60465863   -9.05395321    2.12808426]\n",
      "[   5.00007868  -54.6903567  -113.64708395]\n",
      "[-516.12846108   -3.42298296    4.03275001]\n",
      "[  0.89424462 -37.40010028 -94.62553515]\n",
      "[-100.11023217   -2.1949277   -17.14455575]\n",
      "[  3.51706226 -40.48129471 -98.38678609]\n",
      "[-267.68199966    2.10823033   -1.55383543]\n",
      "[-649.03400607  -26.50669036    3.60849421]\n",
      "[-262.18270725    2.80664102   -3.71027147]\n",
      "[   5.49022371  -55.5230105  -113.48692868]\n",
      "[-351.90601816    4.60558165   -3.19556112]\n",
      "[-465.06706329   -5.81570326    3.8708381 ]\n",
      "[-607.49052318   -4.19133291    3.91674689]\n",
      "[-212.6816598     1.72101323   -3.42165382]\n",
      "[-653.3556658   -10.82691006    3.7761018 ]\n",
      "[-345.114531      3.58680983   -2.52451145]\n",
      "[  5.05656203 -29.85581496 -83.63456667]\n",
      "[-550.18371347   -1.39984958    3.61514085]\n",
      "[  5.62493276 -38.0436734  -95.50931712]\n",
      "[-7.82888710e+02 -1.61696255e+01  7.27469586e-02]\n",
      "[   4.15171048  -42.05642568 -103.64876522]\n",
      "[  4.93056633 -39.03183741 -94.84553822]\n",
      "[-196.59998537    5.14242922   -6.26691808]\n",
      "[-866.51169782   -7.91640467   -0.93707303]\n",
      "[-893.7136416   -13.48633051   -1.35264277]\n",
      "[  6.00022384 -33.48247357 -87.43630838]\n",
      "[  5.18469418 -34.79510411 -88.7700727 ]\n",
      "[  5.6778971  -31.99019158 -82.75669212]\n",
      "[-530.4847216     0.96481069    4.03586855]\n",
      "[-383.13479067   -1.16440227    0.98321192]\n",
      "[-271.07344183    4.96888394   -6.51492948]\n",
      "[  5.83897874 -32.41881191 -83.98454972]\n",
      "[-717.15098545   -4.86710391    1.82257717]\n",
      "[  -1.60751502  -55.74229324 -121.64794168]\n",
      "[-299.56836227    1.75425812   -1.89579793]\n",
      "[ -1.98328183 -20.26106316 -67.57746583]\n",
      "[  5.16162109 -35.60489248 -89.83025375]\n",
      "[  1.64654181 -29.75553397 -80.08856497]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['versicolor', 'versicolor', 'virginica', 'setosa', 'virginica',\n",
       "        'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "        'virginica', 'versicolor', 'versicolor', 'virginica', 'setosa',\n",
       "        'setosa', 'versicolor', 'setosa', 'setosa', 'versicolor',\n",
       "        'setosa', 'versicolor', 'setosa', 'virginica', 'setosa',\n",
       "        'virginica', 'setosa', 'versicolor', 'setosa', 'versicolor',\n",
       "        'virginica', 'versicolor', 'setosa', 'versicolor', 'virginica',\n",
       "        'virginica', 'versicolor', 'virginica', 'versicolor', 'setosa',\n",
       "        'virginica', 'setosa', 'virginica', 'setosa', 'setosa',\n",
       "        'versicolor', 'virginica', 'virginica', 'setosa', 'setosa',\n",
       "        'setosa', 'virginica', 'virginica', 'versicolor', 'setosa',\n",
       "        'virginica', 'setosa', 'versicolor', 'setosa', 'setosa',\n",
       "        'setosa']], dtype='<U10')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:21.970664Z",
     "start_time": "2024-08-15T03:59:21.963317Z"
    }
   },
   "cell_type": "code",
   "source": "fqda2._predict_log_conditionals(test_x)",
   "id": "77de9f7d58e15e62",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.67686068e+02,  8.98064934e-01, -1.13777452e+01],\n",
       "       [-3.10266171e+02,  4.84443575e+00, -4.20726073e-01],\n",
       "       [-3.91251525e+02, -1.09796224e+00,  2.70052269e+00],\n",
       "       [ 3.62296713e+00, -5.27458452e+01, -1.13454179e+02],\n",
       "       [-4.27211426e+02, -2.32554356e+00,  2.94101411e+00],\n",
       "       [-3.46368691e+02, -1.26424642e+00, -2.69449688e+00],\n",
       "       [-2.24629626e+02,  4.61387291e+00, -2.89202058e+00],\n",
       "       [-2.24027815e+02,  2.00126012e+00, -8.59335235e+00],\n",
       "       [-2.31195359e+02,  5.38625066e+00, -4.30841591e+00],\n",
       "       [-6.50646378e+02, -2.30002693e+00,  2.87766313e+00],\n",
       "       [-3.19419483e+02, -1.73522721e-01, -1.82084322e+00],\n",
       "       [-2.41647679e+02,  5.02706153e+00, -4.34865130e+00],\n",
       "       [-1.00848080e+03, -2.21769432e+01, -7.51139885e+00],\n",
       "       [ 5.32995623e+00, -3.77945260e+01, -9.07501462e+01],\n",
       "       [ 6.52254929e+00, -4.64644257e+01, -1.05574096e+02],\n",
       "       [-2.30335807e+02,  3.07200566e+00, -7.03188460e+00],\n",
       "       [ 1.32729158e+00, -7.55209805e+01, -1.49658963e+02],\n",
       "       [ 5.77394601e+00, -4.58875059e+01, -1.07057661e+02],\n",
       "       [-2.96972900e+02,  4.25565011e+00, -6.45363238e+00],\n",
       "       [ 5.67276473e+00, -3.45121081e+01, -8.51145571e+01],\n",
       "       [-3.91636619e+02,  4.15175591e+00, -2.43280053e+00],\n",
       "       [ 1.03010515e+00, -4.31089696e+01, -9.55253730e+01],\n",
       "       [-4.68604659e+02, -9.05395321e+00,  2.12808426e+00],\n",
       "       [ 5.00007868e+00, -5.46903567e+01, -1.13647084e+02],\n",
       "       [-5.16128461e+02, -3.42298296e+00,  4.03275001e+00],\n",
       "       [ 8.94244622e-01, -3.74001003e+01, -9.46255352e+01],\n",
       "       [-1.00110232e+02, -2.19492770e+00, -1.71445558e+01],\n",
       "       [ 3.51706226e+00, -4.04812947e+01, -9.83867861e+01],\n",
       "       [-2.67682000e+02,  2.10823033e+00, -1.55383543e+00],\n",
       "       [-6.49034006e+02, -2.65066904e+01,  3.60849421e+00],\n",
       "       [-2.62182707e+02,  2.80664102e+00, -3.71027147e+00],\n",
       "       [ 5.49022371e+00, -5.55230105e+01, -1.13486929e+02],\n",
       "       [-3.51906018e+02,  4.60558165e+00, -3.19556112e+00],\n",
       "       [-4.65067063e+02, -5.81570326e+00,  3.87083810e+00],\n",
       "       [-6.07490523e+02, -4.19133291e+00,  3.91674689e+00],\n",
       "       [-2.12681660e+02,  1.72101323e+00, -3.42165382e+00],\n",
       "       [-6.53355666e+02, -1.08269101e+01,  3.77610180e+00],\n",
       "       [-3.45114531e+02,  3.58680983e+00, -2.52451145e+00],\n",
       "       [ 5.05656203e+00, -2.98558150e+01, -8.36345667e+01],\n",
       "       [-5.50183713e+02, -1.39984958e+00,  3.61514085e+00],\n",
       "       [ 5.62493276e+00, -3.80436734e+01, -9.55093171e+01],\n",
       "       [-7.82888710e+02, -1.61696255e+01,  7.27469586e-02],\n",
       "       [ 4.15171048e+00, -4.20564257e+01, -1.03648765e+02],\n",
       "       [ 4.93056633e+00, -3.90318374e+01, -9.48455382e+01],\n",
       "       [-1.96599985e+02,  5.14242922e+00, -6.26691808e+00],\n",
       "       [-8.66511698e+02, -7.91640467e+00, -9.37073032e-01],\n",
       "       [-8.93713642e+02, -1.34863305e+01, -1.35264277e+00],\n",
       "       [ 6.00022384e+00, -3.34824736e+01, -8.74363084e+01],\n",
       "       [ 5.18469418e+00, -3.47951041e+01, -8.87700727e+01],\n",
       "       [ 5.67789710e+00, -3.19901916e+01, -8.27566921e+01],\n",
       "       [-5.30484722e+02,  9.64810693e-01,  4.03586855e+00],\n",
       "       [-3.83134791e+02, -1.16440227e+00,  9.83211919e-01],\n",
       "       [-2.71073442e+02,  4.96888394e+00, -6.51492948e+00],\n",
       "       [ 5.83897874e+00, -3.24188119e+01, -8.39845497e+01],\n",
       "       [-7.17150985e+02, -4.86710391e+00,  1.82257717e+00],\n",
       "       [-1.60751502e+00, -5.57422932e+01, -1.21647942e+02],\n",
       "       [-2.99568362e+02,  1.75425812e+00, -1.89579793e+00],\n",
       "       [-1.98328183e+00, -2.02610632e+01, -6.75774658e+01],\n",
       "       [ 5.16162109e+00, -3.56048925e+01, -8.98302537e+01],\n",
       "       [ 1.64654181e+00, -2.97555340e+01, -8.00885650e+01]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:22.014343Z",
     "start_time": "2024-08-15T03:59:22.008759Z"
    }
   },
   "cell_type": "code",
   "source": "fqda1._predict_log_conditionals(test_x)",
   "id": "8774ac603073d8c0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.67686068e+02,  8.98064934e-01, -1.13777452e+01],\n",
       "       [-3.10266171e+02,  4.84443575e+00, -4.20726073e-01],\n",
       "       [-3.91251525e+02, -1.09796224e+00,  2.70052269e+00],\n",
       "       [ 3.62296713e+00, -5.27458452e+01, -1.13454179e+02],\n",
       "       [-4.27211426e+02, -2.32554356e+00,  2.94101411e+00],\n",
       "       [-3.46368691e+02, -1.26424642e+00, -2.69449688e+00],\n",
       "       [-2.24629626e+02,  4.61387291e+00, -2.89202058e+00],\n",
       "       [-2.24027815e+02,  2.00126012e+00, -8.59335235e+00],\n",
       "       [-2.31195359e+02,  5.38625066e+00, -4.30841591e+00],\n",
       "       [-6.50646378e+02, -2.30002693e+00,  2.87766313e+00],\n",
       "       [-3.19419483e+02, -1.73522721e-01, -1.82084322e+00],\n",
       "       [-2.41647679e+02,  5.02706153e+00, -4.34865130e+00],\n",
       "       [-1.00848080e+03, -2.21769432e+01, -7.51139885e+00],\n",
       "       [ 5.32995623e+00, -3.77945260e+01, -9.07501462e+01],\n",
       "       [ 6.52254929e+00, -4.64644257e+01, -1.05574096e+02],\n",
       "       [-2.30335807e+02,  3.07200566e+00, -7.03188460e+00],\n",
       "       [ 1.32729158e+00, -7.55209805e+01, -1.49658963e+02],\n",
       "       [ 5.77394601e+00, -4.58875059e+01, -1.07057661e+02],\n",
       "       [-2.96972900e+02,  4.25565011e+00, -6.45363238e+00],\n",
       "       [ 5.67276473e+00, -3.45121081e+01, -8.51145571e+01],\n",
       "       [-3.91636619e+02,  4.15175591e+00, -2.43280053e+00],\n",
       "       [ 1.03010515e+00, -4.31089696e+01, -9.55253730e+01],\n",
       "       [-4.68604659e+02, -9.05395321e+00,  2.12808426e+00],\n",
       "       [ 5.00007868e+00, -5.46903567e+01, -1.13647084e+02],\n",
       "       [-5.16128461e+02, -3.42298296e+00,  4.03275001e+00],\n",
       "       [ 8.94244622e-01, -3.74001003e+01, -9.46255352e+01],\n",
       "       [-1.00110232e+02, -2.19492770e+00, -1.71445558e+01],\n",
       "       [ 3.51706226e+00, -4.04812947e+01, -9.83867861e+01],\n",
       "       [-2.67682000e+02,  2.10823033e+00, -1.55383543e+00],\n",
       "       [-6.49034006e+02, -2.65066904e+01,  3.60849421e+00],\n",
       "       [-2.62182707e+02,  2.80664102e+00, -3.71027147e+00],\n",
       "       [ 5.49022371e+00, -5.55230105e+01, -1.13486929e+02],\n",
       "       [-3.51906018e+02,  4.60558165e+00, -3.19556112e+00],\n",
       "       [-4.65067063e+02, -5.81570326e+00,  3.87083810e+00],\n",
       "       [-6.07490523e+02, -4.19133291e+00,  3.91674689e+00],\n",
       "       [-2.12681660e+02,  1.72101323e+00, -3.42165382e+00],\n",
       "       [-6.53355666e+02, -1.08269101e+01,  3.77610180e+00],\n",
       "       [-3.45114531e+02,  3.58680983e+00, -2.52451145e+00],\n",
       "       [ 5.05656203e+00, -2.98558150e+01, -8.36345667e+01],\n",
       "       [-5.50183713e+02, -1.39984958e+00,  3.61514085e+00],\n",
       "       [ 5.62493276e+00, -3.80436734e+01, -9.55093171e+01],\n",
       "       [-7.82888710e+02, -1.61696255e+01,  7.27469586e-02],\n",
       "       [ 4.15171048e+00, -4.20564257e+01, -1.03648765e+02],\n",
       "       [ 4.93056633e+00, -3.90318374e+01, -9.48455382e+01],\n",
       "       [-1.96599985e+02,  5.14242922e+00, -6.26691808e+00],\n",
       "       [-8.66511698e+02, -7.91640467e+00, -9.37073032e-01],\n",
       "       [-8.93713642e+02, -1.34863305e+01, -1.35264277e+00],\n",
       "       [ 6.00022384e+00, -3.34824736e+01, -8.74363084e+01],\n",
       "       [ 5.18469418e+00, -3.47951041e+01, -8.87700727e+01],\n",
       "       [ 5.67789710e+00, -3.19901916e+01, -8.27566921e+01],\n",
       "       [-5.30484722e+02,  9.64810693e-01,  4.03586855e+00],\n",
       "       [-3.83134791e+02, -1.16440227e+00,  9.83211919e-01],\n",
       "       [-2.71073442e+02,  4.96888394e+00, -6.51492948e+00],\n",
       "       [ 5.83897874e+00, -3.24188119e+01, -8.39845497e+01],\n",
       "       [-7.17150985e+02, -4.86710391e+00,  1.82257717e+00],\n",
       "       [-1.60751502e+00, -5.57422932e+01, -1.21647942e+02],\n",
       "       [-2.99568362e+02,  1.75425812e+00, -1.89579793e+00],\n",
       "       [-1.98328183e+00, -2.02610632e+01, -6.75774658e+01],\n",
       "       [ 5.16162109e+00, -3.56048925e+01, -8.98302537e+01],\n",
       "       [ 1.64654181e+00, -2.97555340e+01, -8.00885650e+01]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T05:06:39.450188Z",
     "start_time": "2024-08-15T05:06:39.440150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TensorizedLDA(LDA):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # ask plain QDA to fit params\n",
    "        super()._fit_params(X, y)\n",
    "        self.fmt = y.dtype\n",
    "\n",
    "        # stack onto new dimension\n",
    "        self.tensor_inv_cov = np.stack(self.inv_cov)\n",
    "        self.tensor_means = np.stack(self.means)\n",
    "        self.unbiased_by_cov = self.tensor_means.transpose(0,2,1)@self.tensor_inv_cov\n",
    "        \n",
    "    def _predict_log_conditionals(self, x):\n",
    "        return self.unbiased_by_cov@(x-0.5*self.tensor_means)\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        log_cond = self._predict_log_conditionals(x).flatten()\n",
    "        # print('log_cond', log_cond)\n",
    "        log_priors = self.log_a_priori\n",
    "        return np.argmax(log_cond + log_priors)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # this is actually an individual prediction encased in a for-loop\n",
    "        m_obs = X.shape[1]\n",
    "        y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
    "\n",
    "        for i in range(m_obs):\n",
    "            encoded_y_hat_i = self._predict_one(X[:, i].reshape(-1, 1))\n",
    "            y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
    "\n",
    "        # return prediction as a row vector (matching y)\n",
    "        return y_hat.reshape(1, -1)\n",
    "    \n",
    "tlda = TensorizedLDA()\n",
    "tlda.fit(train_x, train_y)\n",
    "tlda.predict(train_x)"
   ],
   "id": "e7c2edf9776afec7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['setosa', 'virginica', 'setosa', 'versicolor', 'versicolor',\n",
       "        'virginica', 'versicolor', 'virginica', 'virginica',\n",
       "        'versicolor', 'versicolor', 'virginica', 'setosa', 'setosa',\n",
       "        'versicolor', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "        'virginica', 'setosa', 'virginica', 'versicolor', 'versicolor',\n",
       "        'versicolor', 'virginica', 'virginica', 'versicolor', 'setosa',\n",
       "        'setosa', 'virginica', 'virginica', 'setosa', 'setosa',\n",
       "        'versicolor', 'versicolor', 'virginica', 'setosa', 'setosa',\n",
       "        'setosa', 'virginica', 'virginica', 'versicolor', 'setosa',\n",
       "        'virginica', 'versicolor', 'virginica', 'setosa', 'virginica',\n",
       "        'virginica', 'setosa', 'setosa', 'virginica', 'virginica',\n",
       "        'virginica', 'versicolor', 'virginica', 'versicolor', 'setosa',\n",
       "        'virginica', 'setosa', 'setosa', 'virginica', 'setosa',\n",
       "        'virginica', 'virginica', 'versicolor', 'setosa', 'versicolor',\n",
       "        'setosa', 'versicolor', 'versicolor', 'virginica', 'versicolor',\n",
       "        'virginica', 'setosa', 'versicolor', 'versicolor', 'setosa',\n",
       "        'versicolor', 'setosa', 'setosa', 'versicolor', 'versicolor',\n",
       "        'versicolor', 'virginica', 'versicolor', 'setosa', 'versicolor',\n",
       "        'virginica']], dtype='<U10')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T05:06:46.968210Z",
     "start_time": "2024-08-15T05:06:43.108525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%timeit\n",
    "tlda.predict(test_x)"
   ],
   "id": "7e837c94a5db8d87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477 μs ± 14.1 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "cell_type": "code",
   "id": "e03b2bbd927e5c41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T05:06:52.203633Z",
     "start_time": "2024-08-15T05:06:52.198708Z"
    }
   },
   "source": [
    "class FasterLDA(TensorizedLDA):\n",
    "\n",
    "    def predict(self, x):        \n",
    "        m_obs = x.shape[1]\n",
    "        k = len(self.log_a_priori)\n",
    "        log_cond = (self.unbiased_by_cov@(x-0.5*self.tensor_means)).reshape(k,m_obs).transpose()\n",
    "        # print(log_cond)\n",
    "        log_priors = self.log_a_priori\n",
    "        y_hat = self.encoder.names[np.argmax( log_priors + log_cond, axis=1)] \n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "flda = FasterLDA()\n",
    "flda.fit(train_x, train_y)\n",
    "_ = flda.predict(train_x)"
   ],
   "outputs": [],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T05:07:13.248315Z",
     "start_time": "2024-08-15T05:07:03.370408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%timeit\n",
    "flda.predict(test_x)"
   ],
   "id": "9aa66e572d054f30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7 μs ± 138 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T05:07:34.154122Z",
     "start_time": "2024-08-15T05:07:34.148911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tlda_predictions = tlda.predict(test_x)\n",
    "flda_predictions = flda.predict(test_x)\n",
    "print(np.bincount(tlda_predictions.flatten() == flda_predictions.flatten()))\n"
   ],
   "id": "9a657c979408561e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 60]\n"
     ]
    }
   ],
   "execution_count": 149
  },
  {
   "cell_type": "code",
   "id": "5f66b46381c398e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:32.020035Z",
     "start_time": "2024-08-15T03:59:22.373473Z"
    }
   },
   "source": [
    "%%timeit\n",
    "tlda.predict(train_x)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.6 μs ± 122 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "f11d4e09c5600da1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T03:59:32.131986Z",
     "start_time": "2024-08-15T03:59:32.128737Z"
    }
   },
   "source": [
    "# %%timeit\n",
    "# flda.predict(train_x)"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "15bfb4a3926e8343",
   "metadata": {},
   "source": [
    "# Preguntas Teoricas\n",
    "### 1. \n",
    "Partimos de la probabilidad condicional $ P(x \\mid y = j) $ para una clase $ j $, que en LDA sigue una distribución normal multivariada:\n",
    "\n",
    "$$\n",
    "f_j(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x - \\mu_j)^T \\Sigma^{-1} (x - \\mu_j)\\right)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "   *  $x$  es un vector de características de dimensión  d ,\n",
    "* $\\mu_j$ es el vector de medias de la clase j ,\n",
    " * $\\Sigma $ es la matriz de covarianza compartida entre las clases.\n",
    "\n",
    "El logaritmo de esta función es:\n",
    "\n",
    "$$\n",
    "\\log f_j(x) = \\log \\left( \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\right) - \\frac{1}{2} (x - \\mu_j)^T \\Sigma^{-1} (x - \\mu_j)\n",
    "$$\n",
    "\n",
    "Este logaritmo se puede descomponer en dos términos:\n",
    "\n",
    "$$\n",
    "\\log f_j(x) = -\\frac{d}{2} \\log(2\\pi) - \\frac{1}{2} \\log |\\Sigma| - \\frac{1}{2} (x - \\mu_j)^T \\Sigma^{-1} (x - \\mu_j)\n",
    "$$\n",
    "\n",
    "El primer término $ -\\frac{d}{2} \\log(2\\pi) $ y el segundo término $ -\\frac{1}{2} \\log |\\Sigma| $ son constantes con respecto a $ x $ y se pueden agrupar en una constante $ C' $:\n",
    "\n",
    "$$\n",
    "\\log f_j(x) = -\\frac{1}{2} (x - \\mu_j)^T \\Sigma^{-1} (x - \\mu_j) + C'\n",
    "$$\n",
    "\n",
    "Luego, se distribuyen los terminos:\n",
    "\n",
    "$$\n",
    "\\log f_j(x) = -\\frac{1}{2} \\left( x^T \\Sigma^{-1} x - 2 \\mu_j^T \\Sigma^{-1} x + \\mu_j^T \\Sigma^{-1} \\mu_j \\right) + C'\n",
    "$$\n",
    "\n",
    "Distribuyendo el $ -\\frac{1}{2} $:\n",
    "\n",
    "$$\n",
    "\\log f_j(x) = -\\frac{1}{2} x^T \\Sigma^{-1} x + \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C'\n",
    "$$\n",
    "\n",
    "Observamos que el término $ -\\frac{1}{2} x^T \\Sigma^{-1} x $ no depende de la clase $ j $, por lo que podemos absorberlo en la constante $ C' $, obteniendo:\n",
    "\n",
    "$$\n",
    "\\log f_j(x) = \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C'\n",
    "$$\n",
    "\n",
    "Finalmente, reordenando los términos:\n",
    "\n",
    "$$\n",
    "\\log f_j(x) = \\mu_j^T \\Sigma^{-1} \\left(x - \\frac{1}{2} \\mu_j\\right) + C'\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa551e5-a088-4e9a-9bda-1cae9ac1aa46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:49.574886Z",
     "start_time": "2024-08-13T02:47:49.572548Z"
    }
   },
   "source": [
    "## 4. Demostración\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{pmatrix}\n",
    "a_{11} & ... & a_{1p} \\\\\n",
    "... & ... & ...\\\\\n",
    "a_{n1} & ... & a_{np} \\\\\n",
    "\\end{pmatrix},\n",
    "B = \\begin{pmatrix}\n",
    "b_{11} & ... & b_{1n} \\\\\n",
    "... & ... & ...\\\\\n",
    "b_{p1} & ... & b_{pn} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "A.B =\n",
    "\\begin{pmatrix}\n",
    "a_{11}b_{11} + ... + a_{1p}b_{p1} & ... & a_{11}b_{1n} + ... + a_{1p}b_{pn} \\\\\n",
    "... & ... & ...\\\\\n",
    "a_{n1}b_{11} + ... + a_{np}b_{p1} & ... & a_{n1}b_{1n} + ... + a_{np}b_{pn}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "diag(A.B) = (a_{11}b_{11} + ... + a_{1p}b_{p1}) + ... + (a_{n1}b_{1n} + ... + a_{np}b_{pn})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "diag(A.B) = \\sum_{k=1}^p a_{1k}b_{k1} + ... + \\sum_{k=1}^p a_{nk}b_{kn} = \\sum_{j=1}^n \\sum_{k=1}^p a_{jk}b_{kj}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "np.sum(\\sum_{k=1}^p a_{jk}b_{kj}, axis=1)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "np.sum(A \\odot B^T, axis=1)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8177d2adf37d3f0",
   "metadata": {},
   "source": [
    "# Ejercicio Teorico\n",
    "## Resolución\n",
    "\n",
    "### Calculo de la salida de la red neuronal\n",
    "\n",
    "Dada la red neuronal de dos capas con los parámetros iniciales:\n",
    "$$\n",
    "w^{(1)} = \\begin{pmatrix}\n",
    "0.1 & -0.5 \\\\\n",
    "-0.3 & -0.9 \\\\\n",
    "0.8 & 0.02\n",
    "\\end{pmatrix}, \\quad\n",
    "b^{(1)} = \\begin{pmatrix}\n",
    "0.1 \\\\\n",
    "0.5 \\\\\n",
    "0.8\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^{(2)} = \\begin{pmatrix}\n",
    "-0.4 & 0.2 & -0.5\n",
    "\\end{pmatrix}, \\quad\n",
    "b^{(2)} = 0.7\n",
    "$$\n",
    "\n",
    "Y la entrada $$ x = \\begin{pmatrix} 1.8 \\\\ -3.4 \\end{pmatrix} $$ \n",
    "Calculamos primero la salida de la primera capa\n",
    "\n",
    "$$\n",
    "z^{(1)} = w^{(1)} \\cdot x + b^{(1)} = \\begin{pmatrix}\n",
    "0.1 & -0.5 \\\\\n",
    "-0.3 & -0.9 \\\\\n",
    "0.8 & 0.02\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1.8 \\\\\n",
    "-3.4\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "0.1 \\\\\n",
    "0.5 \\\\\n",
    "0.8\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1.98 \\\\\n",
    "3.02 \\\\\n",
    "2.172\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Luego, calculamos la activación de la primera capa usando la función sigmoidea:\n",
    "\n",
    "$$\n",
    "a^{(1)} = \\sigma(z^{(1)}) = \\sigma\\left(\\begin{pmatrix}\n",
    "1.98 \\\\\n",
    "3.02 \\\\\n",
    "2.172\n",
    "\\end{pmatrix}\\right) =\n",
    "\\begin{pmatrix}\n",
    "0.8786 \\\\\n",
    "0.9535 \\\\\n",
    "0.8977\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "La salida de la segunda capa es:\n",
    "\n",
    "$$\n",
    "z^{(2)} = w^{(2)} \\cdot a^{(1)} + b^{(2)} = \\begin{pmatrix}\n",
    "-0.4 & 0.2 & -0.5\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "0.8786 \\\\\n",
    "0.9535 \\\\\n",
    "0.8977\n",
    "\\end{pmatrix}\n",
    "+ 0.7 = 0.0903\n",
    "$$\n",
    "\n",
    "Finalmente, la salida final de la red es:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z^{(2)}) = \\sigma(-0.3601) = 0.5226\n",
    "$$\n",
    "\n",
    "\n",
    "### Función de costo\n",
    "\n",
    "La función de costo cuadrática es:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2} (\\hat{y} - y)^2 = \\frac{1}{2} (0.5226 - 5)^2 = -4.4774\n",
    "$$\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "#### Cálculo del error en la capa de salida\n",
    "\n",
    "El error en la capa de salida es:\n",
    "\n",
    "$$\n",
    "\\delta^{(2)} = \\frac{\\partial J}{\\partial \\hat{y}} = (\\hat{y} - y) \\cdot \\sigma'(z^{(2)})\n",
    "$$\n",
    "Con $$ \\sigma'(z) = \\frac{e^{-z}}{(1 + e^{-z})^{2}} $$\n",
    "Entonces:\n",
    "$$\n",
    "\\delta^{(2)} = (0.5226 - 5) \\cdot \\sigma'(0.0903) = -1.1170\n",
    " $$\n",
    "### Gradientes para la segunda capa\n",
    "\n",
    "Las derivadas de la función de costo respecto a los pesos y sesgos de la segunda capa son:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w^{(2)}} = \\delta^{(2)} \\cdot (a^{(1)})^{T} =\n",
    " = \\begin{pmatrix}\n",
    "-0.9916 & -1.0650 & -1.0028\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b^{(2)}} = \\delta^{(2)} = -1.1170\n",
    "$$\n",
    "\n",
    "#### Cálculo del error en la primera capa\n",
    "\n",
    "El error en la primera capa es:\n",
    "\n",
    "$$\n",
    "\\delta^{(1)} = (w^{(2)})^T \\cdot \\delta^{(2)} \\cdot \\sigma'(z^{(1)}) = =\n",
    "\\begin{pmatrix}\n",
    "0.0476 \\\\\n",
    "-0.0099 \\\\\n",
    "0.0512\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Gradientes para la primera capa\n",
    "\n",
    "Las derivadas de la función de costo respecto a los pesos y sesgos de la primera capa son:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w^{(1)}} = \\delta^{(1)} \\cdot (x)^T =  = \\begin{pmatrix}\n",
    "0.0857 & -0.1619 \\\\\n",
    "-0.0178 & 0.00337 \\\\\n",
    "0.0923 & -0.1743\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b^{(1)}} = \\delta^{(1)} = \\begin{pmatrix}\n",
    "0.0476 \\\\\n",
    "-0.0099 \\\\\n",
    "0.0513\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "Los gradientes obtenidos para los pesos y sesgos de cada capa son:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w^{(1)}} = \\begin{pmatrix}\n",
    "0.0857 & -0.1619 \\\\\n",
    "-0.0178 & 0.00337 \\\\\n",
    "0.0923 & -0.1743\n",
    "\\end{pmatrix}, \\quad\n",
    "\\frac{\\partial J}{\\partial b^{(1)}} = \\begin{pmatrix}\n",
    "0.0476 \\\\\n",
    "-0.0099 \\\\\n",
    "0.0513\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w^{(2)}} = \\begin{pmatrix}\n",
    "-0.9916 & -1.0650 & -1.0028\n",
    "\\end{pmatrix}, \\quad\n",
    "\\frac{\\partial J}{\\partial b^{(2)}} = -1.1170\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
