{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3cf4012d20e8b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:37.269489Z",
     "start_time": "2024-08-13T02:47:36.545398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils import ClassEncoder\n",
    "from datasets import get_iris_dataset\n",
    "X_full_iris, y_full_iris = get_iris_dataset()\n",
    "print(X_full_iris.shape)\n",
    "print(y_full_iris.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83ef9c952b426af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:37.296264Z",
     "start_time": "2024-08-13T02:47:37.291355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de clases:\n",
      "setosa: 0.3333\n",
      "versicolor: 0.3333\n",
      "virginica: 0.3333\n"
     ]
    }
   ],
   "source": [
    "def print_distribution(_y):\n",
    "    encoder = ClassEncoder()\n",
    "    encoded_y = encoder.fit_transform(_y) # convert class to number (encode)\n",
    "    print('Distribución de clases:')\n",
    "    distribution = np.bincount(encoded_y.flatten())/len(encoded_y)\n",
    "    for class_name, value in zip(encoder.names, distribution):\n",
    "        print(f'{class_name}: {value:.4f}')\n",
    "\n",
    "print_distribution(y_full_iris)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767fec1ed536e345",
   "metadata": {},
   "source": [
    "Se observa que el dataset Iris se encuentra balanceado, es decir que no hay alguna preponderancia de alguna de las clases por sobre las demás."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6de399ab132490",
   "metadata": {},
   "source": [
    "### 1) QDA Entrenado con:  probabilidades a priori uniforme y  una clase con probabilidad 0.9, las demás 0.05 ( 3 combinaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fbc6833c5a1e73b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:37.480657Z",
     "start_time": "2024-08-13T02:47:37.443616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A prioris:\n",
      " setosa:0.333 , versicolor:0.333 , virginica:0.333 \n",
      "[Model 0] Train (apparent) error is 0.0222 while test error is 0.0167\n",
      "A prioris:\n",
      " setosa:0.900 , versicolor:0.050 , virginica:0.050 \n",
      "[Model 1] Train (apparent) error is 0.0222 while test error is 0.0167\n",
      "A prioris:\n",
      " setosa:0.050 , versicolor:0.900 , virginica:0.050 \n",
      "[Model 2] Train (apparent) error is 0.0333 while test error is 0.0000\n",
      "A prioris:\n",
      " setosa:0.050 , versicolor:0.050 , virginica:0.900 \n",
      "[Model 3] Train (apparent) error is 0.0333 while test error is 0.0500\n"
     ]
    }
   ],
   "source": [
    "from utils import split_transpose, QDA, accuracy\n",
    "\n",
    "def priori_test(dataset):\n",
    "    X_full, y_full = dataset\n",
    "    a_priori_A = [1/3, 1/3, 1/3] # modelo 0\n",
    "    a_priori_B_1 = [0.9, 0.05, 0.05] # modelo 1\n",
    "    a_priori_B_2 = [0.05, 0.9, 0.05] # modelo 2\n",
    "    a_priori_B_3 = [0.05, 0.05, 0.9] # modelo 3\n",
    "    \n",
    "    a_priori_list = [a_priori_A, a_priori_B_1, a_priori_B_2, a_priori_B_3]\n",
    "    # from utils import QDA\n",
    "    # rng_seed = 6543\n",
    "    train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, 6543)\n",
    "    \n",
    "    for i,_a_priori in enumerate(a_priori_list):\n",
    "        model = QDA()\n",
    "        model.fit(train_x, train_y, _a_priori)\n",
    "        print('A prioris:')\n",
    "        print(\",\".join([f' {class_name}:{p:.3f} ' for class_name, p in zip(model.encoder.names, _a_priori)]))\n",
    "        train_acc = accuracy(train_y, model.predict(train_x))\n",
    "        test_acc = accuracy(test_y, model.predict(test_x))\n",
    "        print(f\"[Model {i}] Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")\n",
    "        # print('\\n')\n",
    "\n",
    "priori_test(get_iris_dataset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22fb5b463cbbbd",
   "metadata": {},
   "source": [
    "A partir de estos datos y dejando de lado cuál es la verdadera distribución, se podrían hacer las siguientes suposiciones:\n",
    "- El modelo 0 y el modelo 1 parecerían cometer el mismo grado de error al hacer dichas suposiciones sobre los priors.\n",
    "- El modelo 2 parecería sobreajustar (hay overfitting) a los datos de test.\n",
    "- El modelo 3 tiene un mayor error tanto en el entrenamiento como en la prueba, lo que indica que no logra generalizar y que dichos priors tienen un efecto detrimental en la performance del modelo.\n",
    "\n",
    "Se utilizará la versión de QDA provista por SKLearn (a pesar de las diferencias en su implementación) a modo de evaluar cualitativamente el efecto de los priors sobre éste dataset y confirmar las suposiciones ya mencionadas, utilizando Cross Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eb276f0-7635-4c7f-9eb5-7017aa69781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with priors [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]: 0.9800000000000001 ± 0.02666666666666666 for model 0\n",
      "Accuracy with priors [0.9, 0.05, 0.05]: 0.9800000000000001 ± 0.02666666666666666 for model 1\n",
      "Accuracy with priors [0.05, 0.9, 0.05]: 0.9733333333333334 ± 0.024944382578492935 for model 2\n",
      "Accuracy with priors [0.05, 0.05, 0.9]: 0.9666666666666668 ± 0.036514837167011066 for model 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def priori_test_cross_val(dataset):\n",
    "    X_full, y_full = dataset\n",
    "    a_priori_A = [1/3, 1/3, 1/3] # modelo 0\n",
    "    a_priori_B_1 = [0.9, 0.05, 0.05] # modelo 1\n",
    "    a_priori_B_2 = [0.05, 0.9, 0.05] # modelo 2\n",
    "    a_priori_B_3 = [0.05, 0.05, 0.9] # modelo 3\n",
    "        \n",
    "    a_priori_list = [a_priori_A, a_priori_B_1, a_priori_B_2, a_priori_B_3]\n",
    "    \n",
    "    for i, _priors in enumerate(a_priori_list):\n",
    "        qda = QuadraticDiscriminantAnalysis(priors=_priors) #utilizo QDA de sklearn para poder utilizar cross_val_score sin problemas\n",
    "        cv_scores = cross_val_score(qda, X_full, y_full.flatten(), cv=5)  # 5-fold cross-validation\n",
    "        print(f\"Accuracy with priors {_priors}: {cv_scores.mean()} ± {cv_scores.std()} for model {i}\")\n",
    "\n",
    "priori_test_cross_val(get_iris_dataset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13072daf-0fbe-497f-8f5e-91b3e3c53547",
   "metadata": {},
   "source": [
    "Aquí se ve que el resultado del modelo 2 anterior no era representativo. La precisión es ligeramente menor que en los modelos anteriores, lo que podría indicar que las muestras de la segunda clase no son tan fácilmente separables como las de la primera clase.\n",
    "\n",
    "En cuanto al modelo 3: presenta la precisión más baja y la desviación estándar más alta, lo que podría sugerir que la tercera clase es la más difícil de clasificar correctamente o que el sesgo hacia esa clase introduce más incertidumbre en las predicciones.\n",
    "\n",
    "Con respecto al modelo 0 y 1: cabe la posibilidad de pensar que por ejemplo, las características que definen la primera clase son muy distintas, el modelo podría seguir clasificando correctamente la mayoría de las instancias de esa clase, incluso si los priors cambian. Ello querría decir que dicha clase es fácilmente separable de las demás. Para obtener una visión más clara de qué sucediendo entre el modelo 1 y el 0, se puede ver la matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceafba33-5913-469f-8826-05d30f7b8ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión de prueba para el modelo 0:\n",
      "[[23  0  0]\n",
      " [ 0 20  1]\n",
      " [ 0  0 16]]\n",
      "Matriz de confusión de prueba para el modelo 1:\n",
      "[[23  0  0]\n",
      " [ 0 20  1]\n",
      " [ 0  0 16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(model, test_x, test_y):\n",
    "    test_preds_0 = model.predict(test_x)\n",
    "    return confusion_matrix(test_y.T, test_preds_0.T)\n",
    "\n",
    "\n",
    "X_full, y_full = get_iris_dataset()\n",
    "a_priori_A = [1/3, 1/3, 1/3] # modelo 0\n",
    "a_priori_B_1 = [0.9, 0.05, 0.05] # modelo 1\n",
    "a_priori_B_2 = [0.05, 0.9, 0.05] # modelo 2\n",
    "a_priori_B_3 = [0.05, 0.05, 0.9] # modelo 3\n",
    "\n",
    "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, 6543)\n",
    "\n",
    "model = QDA()\n",
    "\n",
    "model.fit(train_x, train_y, a_priori_A)\n",
    "print(\"Matriz de confusión de prueba para el modelo 0:\")\n",
    "print(get_cm(model, test_x, test_y))\n",
    "\n",
    "model.fit(train_x, train_y, a_priori_B_1)\n",
    "print(\"Matriz de confusión de prueba para el modelo 1:\")\n",
    "print(get_cm(model, test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8091cf-a134-4179-9f00-70a9588c29cd",
   "metadata": {},
   "source": [
    "El hecho de que el modelo 1, con priors sesgados ([0.9, 0.05, 0.05]), produzca una matriz de confusión similar indica que la información proporcionada por los datos (en este caso de la clase 1) es lo suficientemente fuerte como para compensar el sesgo de los priors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4691e2c3b5a69f5",
   "metadata": {},
   "source": [
    "## 2) Repetir punto 1 para el dataset penguin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4263ac5f7974baf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:37.772793Z",
     "start_time": "2024-08-13T02:47:37.503796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de clases:\n",
      "Adelie: 0.4415\n",
      "Chinstrap: 0.1988\n",
      "Gentoo: 0.3596\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_penguins\n",
    "X_full_penguin, y_full_penguin = get_penguins()\n",
    "\n",
    "print_distribution(y_full_penguin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a07fb8018fb6aca",
   "metadata": {},
   "source": [
    "Se puede observar este dataset no está balanceado con respecto a la cantidad de datos por clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44318e84b2b475f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:37.862391Z",
     "start_time": "2024-08-13T02:47:37.790629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A prioris:\n",
      " Adelie:0.333 , Chinstrap:0.333 , Gentoo:0.333 \n",
      "[Model 0] Train (apparent) error is 0.0098 while test error is 0.0073\n",
      "A prioris:\n",
      " Adelie:0.900 , Chinstrap:0.050 , Gentoo:0.050 \n",
      "[Model 1] Train (apparent) error is 0.0195 while test error is 0.0219\n",
      "A prioris:\n",
      " Adelie:0.050 , Chinstrap:0.900 , Gentoo:0.050 \n",
      "[Model 2] Train (apparent) error is 0.0098 while test error is 0.0219\n",
      "A prioris:\n",
      " Adelie:0.050 , Chinstrap:0.050 , Gentoo:0.900 \n",
      "[Model 3] Train (apparent) error is 0.0098 while test error is 0.0073\n"
     ]
    }
   ],
   "source": [
    "priori_test(get_penguins())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c4be3-c9b4-4961-98d1-cce5ee6e7dbf",
   "metadata": {},
   "source": [
    "Los modelos que mejor generalizan son el 0 y el 3. Parece ser un caso análogo al de Iris en cuanto a lo que sucede con diferentes priors. Se intentará verificar a continuación si esto es así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44fc8bf0-47e6-49ac-a9ed-be8dc0aac115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with priors [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]: 0.9882779198635976 ± 0.010993807100854342 for model 0\n",
      "Accuracy with priors [0.9, 0.05, 0.05]: 0.9824381926683717 ± 0.005925745287640952 for model 1\n",
      "Accuracy with priors [0.05, 0.9, 0.05]: 0.9589514066496164 ± 0.025287380244108995 for model 2\n",
      "Accuracy with priors [0.05, 0.05, 0.9]: 0.9882779198635976 ± 0.010993807100854342 for model 3\n"
     ]
    }
   ],
   "source": [
    "priori_test_cross_val(get_penguins())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fd69f-9450-4896-bd05-152e3f286175",
   "metadata": {},
   "source": [
    "Se puede ahora notar nuevamente que los modelos 1 (aunque por muy poco) y 2 son los que peor performan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ff41f08-0b70-41cb-9202-6e841535310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión de prueba para el modelo 0:\n",
      "[[67  0  0]\n",
      " [ 1 28  0]\n",
      " [ 0  0 41]]\n",
      "Matriz de confusión de prueba para el modelo 1:\n",
      "[[67  0  0]\n",
      " [ 3 26  0]\n",
      " [ 0  0 41]]\n",
      "Matriz de confusión de prueba para el modelo 2:\n",
      "[[64  3  0]\n",
      " [ 0 29  0]\n",
      " [ 0  0 41]]\n",
      "Matriz de confusión de prueba para el modelo 3:\n",
      "[[67  0  0]\n",
      " [ 1 28  0]\n",
      " [ 0  0 41]]\n"
     ]
    }
   ],
   "source": [
    "X_full, y_full = get_penguins()\n",
    "\n",
    "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, 6543)\n",
    "\n",
    "model = QDA()\n",
    "\n",
    "model.fit(train_x, train_y, a_priori_A)\n",
    "print(\"Matriz de confusión de prueba para el modelo 0:\")\n",
    "print(get_cm(model, test_x, test_y))\n",
    "\n",
    "model.fit(train_x, train_y, a_priori_B_1)\n",
    "print(\"Matriz de confusión de prueba para el modelo 1:\")\n",
    "print(get_cm(model, test_x, test_y))\n",
    "\n",
    "model.fit(train_x, train_y, a_priori_B_2)\n",
    "print(\"Matriz de confusión de prueba para el modelo 2:\")\n",
    "print(get_cm(model, test_x, test_y))\n",
    "\n",
    "model.fit(train_x, train_y, a_priori_B_3)\n",
    "print(\"Matriz de confusión de prueba para el modelo 3:\")\n",
    "print(get_cm(model, test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae2fde-b3b1-4b48-94e4-5d86fc6da0c1",
   "metadata": {},
   "source": [
    "De aquí se puede concluir que existe una relación de compromiso entre clasificar correctamente las muestras de la primera y segunda clase. El límite de decisión que genera un error 0 requiera probablemente de mayor complejidad (trayendo aparejado un posible problema de overfitting) o siendo imposible de lograr.\n",
    "\n",
    "El modelo 0 cuyos priors son los que más se asemejan a los reales, es uno de los que menor performa. El modelo 3 da resultados similares; ello se debe a que la tercera clase es evidentemente es fácilmente separable de las demás (puesto a que todos los modelos, a pesar de la abismal diferencia entre priors, han sido capaz de clasificarlos correctamente) y por lo tanto, la información proporcionada por los datos es lo suficientemente fuerte como para compensar el sesgo del prior (análogo al dataset anterior)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b5275361fad988",
   "metadata": {},
   "source": [
    "### 3) Implementar LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bf258ed0b1a69a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:37.936316Z",
     "start_time": "2024-08-13T02:47:37.932076Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import BaseBayesianClassifier, inv, det\n",
    "\n",
    "class LDA(BaseBayesianClassifier):\n",
    "    # Utiliza una matriz de covarianza ponderada para evitar que clases con mayor cantidad de muestras tengan mayor influencia en su valor final. \n",
    "    # Al ponderar, se intenta que la variabilidad dentro de la clase más pequeña también sea tenida en cuenta de manera proporcional.\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # Inicializa la matriz de covarianza ponderada\n",
    "    cov_matrix = np.zeros((X.shape[0], X.shape[0]))\n",
    "    \n",
    "    # Suma la covarianza de cada clase ponderada por su tamaño\n",
    "    for idx in range(len(self.log_a_priori)):\n",
    "        X_class = X[:, y.flatten() == idx]\n",
    "        cov_matrix += np.cov(X_class, bias=True) * (X_class.shape[1] - 1)\n",
    "    \n",
    "    # Divide por el número total de muestras (menos 1 para corrección de Bessel)\n",
    "    cov_matrix /= (X.shape[1] - len(self.log_a_priori))\n",
    "    \n",
    "    # Calcula la inversa de la matriz de covarianza ponderada\n",
    "    self.inv_cov = inv(cov_matrix)\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True) for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "    return 0.5*np.log(det(self.inv_cov)) -0.5 * unbiased_x.T @ self.inv_cov @ unbiased_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3349622de5cd6e",
   "metadata": {},
   "source": [
    "Se comparar LDA vs QDA (Sin multiples prioris, es decir que se estimen a partir de los datos) con los dos datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5afccf7c21a27a1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:38.043952Z",
     "start_time": "2024-08-13T02:47:37.984166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset=iris][Model=QDA] train err 0.0111, test err 0.0167\n",
      "[Dataset=iris][Model=LDA] train err 0.0222, test err 0.0167\n",
      "[Dataset=penguins][Model=QDA] train err 0.0146, test err 0.0146\n",
      "[Dataset=penguins][Model=LDA] train err 0.0098, test err 0.0146\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset in zip(['iris', 'penguins'], [get_iris_dataset(), get_penguins()]):\n",
    "    for model_name, curr_model in zip(['QDA', 'LDA'], [QDA, LDA]):\n",
    "        model = curr_model()\n",
    "        x_full, y_full = dataset\n",
    "        train_x, train_y, test_x, test_y = split_transpose(x_full, y_full, 0.4, 6543)\n",
    "        model.fit(train_x, train_y)\n",
    "        train_acc = accuracy(train_y, model.predict(train_x))\n",
    "        test_acc = accuracy(test_y, model.predict(test_x))\n",
    "        print(f\"[Dataset={dataset_name}][Model={model_name}] train err {1-train_acc:.4f}, test err {1-test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d4222bfffcc0c",
   "metadata": {},
   "source": [
    "## FALTA CONCLUSIÓN!!!\n",
    "\n",
    "Aclarar que QDA en general necesita más datos que LDA y que por eso quizás acá, no tenga mucho impacto el considerar cada matriz de covarianza por clase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c4b6726cf49c16",
   "metadata": {},
   "source": [
    "### 4) Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Las conclusiones previas se mantienen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a470151e2b2562f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:38.360621Z",
     "start_time": "2024-08-13T02:47:38.086970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Dataset Model  seed  Error (train)  Error (test)\n",
      "0       iris   QDA  6543       0.011111      0.016667\n",
      "1       iris   QDA  5501       0.022222      0.016667\n",
      "2       iris   QDA   125       0.022222      0.016667\n",
      "3       iris   LDA  6543       0.022222      0.016667\n",
      "4       iris   LDA  5501       0.022222      0.016667\n",
      "5       iris   LDA   125       0.011111      0.016667\n",
      "6   penguins   QDA  6543       0.014634      0.014599\n",
      "7   penguins   QDA  5501       0.014634      0.007299\n",
      "8   penguins   QDA   125       0.009756      0.014599\n",
      "9   penguins   LDA  6543       0.009756      0.014599\n",
      "10  penguins   LDA  5501       0.009756      0.014599\n",
      "11  penguins   LDA   125       0.014634      0.007299\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for dataset_name,dataset in zip(['iris', 'penguins'],[get_iris_dataset(), get_penguins()]):\n",
    "    for model_name, curr_model in zip(['QDA', 'LDA'],[QDA, LDA]):\n",
    "        for seed in [6543, 5501,125]:\n",
    "            model = curr_model()\n",
    "            x_full, y_full = dataset\n",
    "            train_x, train_y, test_x, test_y = split_transpose(x_full, y_full,test_sz=0.4, random_state=seed)\n",
    "            model.fit(train_x, train_y)\n",
    "            train_acc = accuracy(train_y, model.predict(train_x))\n",
    "            test_acc = accuracy(test_y, model.predict(test_x))\n",
    "            # print(f\"[Dataset={dataset_name}][Model={model_name}] train err {1-train_acc:.4f}, test err {1-test_acc:.4f}\")\n",
    "            row = {\n",
    "                'Dataset': dataset_name,\n",
    "                'Model': model_name,\n",
    "                'seed': seed,\n",
    "                'Error (train)': 1-train_acc,\n",
    "                'Error (test)': 1-test_acc,\n",
    "            }\n",
    "            \n",
    "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c9f73-e241-4326-aec7-576bf4d47d14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:38.394522Z",
     "start_time": "2024-08-13T02:47:38.391794Z"
    }
   },
   "source": [
    "## FALTA CONCLUSIÓN!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a8736a93b6bc1",
   "metadata": {},
   "source": [
    "### 5) Tensorized QDA vs QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9d5e4c3036eca76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:38.459553Z",
     "start_time": "2024-08-13T02:47:38.448819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0111 while test error is 0.0167\n"
     ]
    }
   ],
   "source": [
    "from utils import TensorizedQDA\n",
    "\n",
    "x_full, y_full = get_iris_dataset()\n",
    "train_x, train_y, test_x, test_y = split_transpose(x_full, y_full,test_sz=0.4, random_state=6543)\n",
    "\n",
    "tqda = TensorizedQDA()\n",
    "tqda.fit(train_x, train_y)\n",
    "train_acc = accuracy(train_y, tqda.predict(train_x))\n",
    "test_acc = accuracy(test_y, tqda.predict(test_x))\n",
    "\n",
    "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab0347e9246ad3bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:46.816203Z",
     "start_time": "2024-08-13T02:47:38.552319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "860 μs ± 6.29 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "tqda.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7970b3217fa7540",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:46.902856Z",
     "start_time": "2024-08-13T02:47:46.890697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0111 while test error is 0.0167\n"
     ]
    }
   ],
   "source": [
    "qda = QDA()\n",
    "qda.fit(train_x, train_y)\n",
    "train_acc = accuracy(train_y, qda.predict(train_x))\n",
    "test_acc = accuracy(test_y, qda.predict(test_x))\n",
    "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fae0163b9d8c14f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:49.298448Z",
     "start_time": "2024-08-13T02:47:47.013962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.48 ms ± 49.7 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "qda.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f685564-8127-4da0-8872-a0aaa847936c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:49.379366Z",
     "start_time": "2024-08-13T02:47:49.377086Z"
    }
   },
   "source": [
    "Tensorized QDA aprovecha que varias de las operaciones que se tienen que realizar se pueden escribir de forma matricial, pudiendo realizarse de forma paralela. Esto se hace evidente en las siguientes líneas:\n",
    "\n",
    "```\n",
    "self.tensor_inv_cov = np.stack(self.inv_covs)\n",
    "self.tensor_means = np.stack(self.means)\n",
    "```\n",
    "\n",
    "Lo cual evidencia que las cuentas serán hechas simultáneamente, de forma paralela, para todas las clases y luego se verifica en la firma de la función:\n",
    "```\n",
    "def _predict_log_conditional(self, x, class_idx)\n",
    "```\n",
    "\n",
    "Ya que no es necesario especificar para qué clase será hecha dicha predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88475da94ee1081",
   "metadata": {},
   "source": [
    "## Faster QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a465a592ed10a61d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T04:39:21.806855Z",
     "start_time": "2024-08-13T04:39:21.798014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 90)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m fqda \u001b[38;5;241m=\u001b[39m FasterQDA()\n\u001b[0;32m     41\u001b[0m fqda\u001b[38;5;241m.\u001b[39mfit(train_x, train_y)\n\u001b[1;32m---> 42\u001b[0m fqda\u001b[38;5;241m.\u001b[39mpredict(train_x)\n",
      "Cell \u001b[1;32mIn[17], line 34\u001b[0m, in \u001b[0;36mFasterQDA.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape)        \n\u001b[1;32m---> 34\u001b[0m     log_cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_log_conditionals(x) \u001b[38;5;66;03m# (n,k)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     log_priors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_a_priori \u001b[38;5;66;03m#(k,)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mnames[np\u001b[38;5;241m.\u001b[39margmax(log_priors \u001b[38;5;241m+\u001b[39m log_cond, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "from utils import TensorizedQDA\n",
    "class FasterQDA(TensorizedQDA):\n",
    "    \n",
    "    def augment_dim(self, vec):\n",
    "        return np.array([np.repeat(vec[i], 90) for i in range(len(self.log_a_priori))])\n",
    "        \n",
    "    \n",
    "    def _predict_log_conditionals(self, x):\n",
    "        # train_x era de 4x90\n",
    "        # despues de hacer fit -> x es un tensor de 3 x 4 x 90 (o sea mismo array, 3 veces)\n",
    "        # (hacemos la prediccion para las 3 clases en conjunto)\n",
    "        unbiased_x = x - self.tensor_means\n",
    "        # unbiased_x.shape = k x p x n \n",
    "        print(unbiased_x.shape)\n",
    "        inner_prod = unbiased_x.transpose(0, 2, 1) @ self.tensor_inv_cov @ unbiased_x\n",
    "        # inner prod shape: (k, n, n) -> k matrices de nxn\n",
    "        # debemos de pensar que como es un producto de tensores => \n",
    "        # cada producto produce una matriz de nxn\n",
    "        \n",
    "        # dado un k fijo (estando parados en una clase)\n",
    "        # \n",
    "        \n",
    "        # podemos imaginar que en cada matriz\n",
    "        \n",
    "        # obtengo diagonal para cada matriz,\n",
    "        # ya que solo las que son consigo mismas son las que nos interesan (producto interno)\n",
    "        new_mat = np.array([np.diag(mat) for mat in inner_prod]) # (k,n)\n",
    "        itcov = np.log(det(self.tensor_inv_cov)) # (k,)\n",
    "        \n",
    "        # truco para no tener que crear un nuevo vector itcov de (k,n) para poder sumarlo\n",
    "        return 0.5 * itcov - 0.5 * new_mat.transpose() # (n,k)\n",
    "    def predict(self, X):\n",
    "        print(X.shape)        \n",
    "        log_cond = self._predict_log_conditionals(x) # (n,k)\n",
    "        log_priors = self.log_a_priori #(k,)\n",
    "        y_hat = self.encoder.names[np.argmax(log_priors + log_cond, axis=1)]\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "fqda = FasterQDA()\n",
    "fqda.fit(train_x, train_y)\n",
    "fqda.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ddbeb2b41026f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:49.528525Z",
     "start_time": "2024-08-13T02:47:49.523652Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 90 # obs\n",
    "p = 4 # features\n",
    "k = 3 # classes\n",
    "i_cov = np.ones((k,p,p)) # no se puede tocar\n",
    "x = np.ones((k,p,n))\n",
    "first = x.transpose(0,2,1)@i_cov\n",
    "print(f'{(x.transpose(0,2,1)).shape} x {i_cov.shape} = {first.shape}')\n",
    "print(f'{first.shape} x {x.shape} = {(first@x).shape} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813d973d2121aeba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:49.574886Z",
     "start_time": "2024-08-13T02:47:49.572548Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc31b904b08858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T02:47:49.651510Z",
     "start_time": "2024-08-13T02:47:49.649460Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
